<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cockroachdb on Alfonso Subiotto</title><link>http://asubiotto.com/tags/cockroachdb/</link><description>Recent content in Cockroachdb on Alfonso Subiotto</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 30 Jun 2020 00:00:00 +0000</lastBuildDate><atom:link href="http://asubiotto.com/tags/cockroachdb/index.xml" rel="self" type="application/rss+xml"/><item><title>Disk Spilling in a Vectorized Execution Engine</title><link>http://asubiotto.com/writing/disk-spilling-vectorized-execution-engine/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>http://asubiotto.com/writing/disk-spilling-vectorized-execution-engine/</guid><description>&lt;p>Late last year, we shipped v1 of our &lt;a href="https://www.cockroachlabs.com/docs/stable/vectorized-execution">vectorized execution engine&lt;/a>. It enables column-based query execution and speeds up complex joins and aggregations, improving analytical capabilities in CockroachDB (which is first and foremost optimized for OLTP workloads). v1 of the engine didn’t support disk spilling, which meant it couldn’t execute &lt;a href="https://www.cockroachlabs.com/docs/stable/vectorized-execution/#disk-spilling-operations">certain memory-intensive queries&lt;/a> if there was not enough memory available. Starting in &lt;a href="https://www.cockroachlabs.com/product/whats-new/">CockroachDB v20.1&lt;/a>, these queries fall back to disk (also known as “spilling” to disk).&lt;/p></description></item><item><title>How We Built a Vectorized Execution Engine</title><link>http://asubiotto.com/writing/how-we-built-vectorized-execution-engine/</link><pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate><guid>http://asubiotto.com/writing/how-we-built-vectorized-execution-engine/</guid><description>&lt;p>CockroachDB is an &lt;a href="https://en.wikipedia.org/wiki/Online_transaction_processing">OLTP&lt;/a> database, specialized for serving high-throughput queries that read or write a small number of rows. As we gained more usage, we found that customers weren’t getting the performance they expected from analytic queries that read a lot of rows, like large scans, joins, or aggregations. In April 2018, we started to seriously investigate how to improve the performance of these types of queries in CockroachDB, and began working on a new SQL execution engine. In this blog post, we use example code to discuss how we built the new engine and why it results in up to a 4x speed improvement on an industry-standard benchmark.&lt;/p></description></item></channel></rss>